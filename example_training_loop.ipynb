{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from src import graphconv\n",
    "from src import gcw\n",
    "from src import processtools as pt\n",
    "from src import healpyfunctional as hpf\n",
    "\n",
    "import healpy as hp\n",
    "\n",
    "from pygsp.graphs import SphereHealpix\n",
    "from pygsp import filters\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(tf.config.get_visible_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNHealpy(Model, gcw.GCW): #model definition. the class handles calling the appropriate layers and calculating \n",
    "                                 #the graph laplacians.    \n",
    "    \"\"\"\n",
    "    Graph convolutional NN models for the healpy pixelization scheme. \n",
    "    Precalculates the polynomial approximation of the graph laplacian for graph convolutional layers.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 nside,\n",
    "                 indices,\n",
    "                 channels=1,\n",
    "                 use_polyK=False,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        :param nside: nside of the input maps\n",
    "        :param indices: indices of the input maps\n",
    "        :param channels: number of input channels\n",
    "        :param use_polyK: Bool. Optional. If True, will precalculate P(L) and use P(L) in graph convolution\n",
    "                          layers. Might lead to performance gains.\n",
    "        \"\"\"\n",
    "        super(GCNHealpy, self).__init__(name='') #we are goint super init again after we have everything\n",
    "        self.nside = nside\n",
    "        self.indices = indices\n",
    "        self.channels = channels\n",
    "        self.use_polyK = use_polyK\n",
    "        self.verbose = verbose\n",
    "        self.polydict = {}\n",
    "        self.Ldict = {}\n",
    "    \n",
    "    def l2(self, weight_decay):\n",
    "        return tf.keras.regularizers.L2(l2=weight_decay)\n",
    "        \n",
    "    def model(self, weight_decay, sdrate, include_top=True, num_classes=3):\n",
    "        #definition of the network. this is a method under the GCNHealpy model class.\n",
    "        #functional API is easier to use compared to subclassing every layer in my opinion.\n",
    "        \"\"\"\n",
    "        :param weight_decay: l2 regularization penalty to apply on the convolution kernels\n",
    "        :param sdrate: spatial dropout rate to apply after the convolution layers\n",
    "        :param include_top: if true, will include the globalavereagepooling and densely connected layers\n",
    "        :param num_classes: number of outputs of the final densely connected layer.\n",
    "        \"\"\"\n",
    "        inputs = tf.keras.layers.Input(shape=(len(self.indices), self.channels), name=\"input_maps\")\n",
    "        x1 = self.Conv(nside=self.nside, indices=self.indices, n_neighbors=8, poly_type='chebyshev',\n",
    "                       K=4, Fout=32, activation='relu', use_bn=True, \n",
    "                       kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(inputs)\n",
    "        x1 = self.Conv(nside=self.nside, indices=self.indices, n_neighbors=8, poly_type='chebyshev',\n",
    "                       K=4, Fout=32, activation='relu', use_bn=True, \n",
    "                       kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x1)\n",
    "        x1 = hpf.HealpyPseudoConv(p=1, Fout=32, activation='relu', initializer='he_normal',\n",
    "                                  kernel_regularizer=self.l2(weight_decay), nside=self.nside, \n",
    "                                  indices=self.indices)(x1)\n",
    "        x1 = tf.keras.layers.SpatialDropout1D(sdrate)(x1)        \n",
    "        x1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                epsilon=0.001, center=False, \n",
    "                                                scale=False)(x1) \n",
    "\n",
    "        \n",
    "        x2 = self.Conv(nside=self.nside, indices=self.indices, n_neighbors=20, poly_type='chebyshev',\n",
    "                       K=8, Fout=64, activation='relu', use_bn=True, \n",
    "                       kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(inputs)\n",
    "        x2 = hpf.HealpyPseudoConv(p=1, Fout=64, activation='relu', initializer='he_normal',\n",
    "                                  kernel_regularizer=self.l2(weight_decay), nside=self.nside, \n",
    "                                  indices=self.indices)(x2)\n",
    "        x2 = tf.keras.layers.SpatialDropout1D(sdrate)(x2)\n",
    "        x2 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                epsilon=0.001, center=False, \n",
    "                                                scale=False)(x2)\n",
    "        \n",
    "        x3 = self.Conv(nside=self.nside, indices=self.indices, n_neighbors=20, poly_type='chebyshev',\n",
    "                       K=12, Fout=32, activation='relu', use_bn=True, \n",
    "                       kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(inputs)\n",
    "        x3 = hpf.HealpyPseudoConv(p=1, Fout=32, activation='relu', initializer='he_normal',\n",
    "                                  kernel_regularizer=self.l2(weight_decay), nside=self.nside, \n",
    "                                  indices=self.indices)(x3)\n",
    "        x3 = tf.keras.layers.SpatialDropout1D(sdrate)(x3)\n",
    "        x3 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                epsilon=0.001, center=False, \n",
    "                                                scale=False)(x3) \n",
    "\n",
    "        \n",
    "        nside_out1 = hpf.HealpyPseudoConv(p=1, Fout=32, activation='relu', \n",
    "                                          initializer='he_normal',\n",
    "                                          kernel_regularizer=self.l2(weight_decay),\n",
    "                                          nside=self.nside, indices=self.indices).nside_out\n",
    "        indices_out1 = hpf.HealpyPseudoConv(p=1, Fout=32, activation='relu', \n",
    "                                            initializer='he_normal',\n",
    "                                            kernel_regularizer=self.l2(weight_decay),\n",
    "                                            nside=self.nside, indices=self.indices).indices_out\n",
    "        \n",
    "        x = tf.keras.layers.Concatenate(axis=-1)([x1,x2,x3])        \n",
    "        \n",
    "        x1 = self.Conv(nside=nside_out1, indices=indices_out1, n_neighbors=20, poly_type='chebyshev',\n",
    "                       K=4, Fout=64, activation='relu', use_bn=True, \n",
    "                       kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x)\n",
    "        x1 = self.Conv(nside=nside_out1, indices=indices_out1, n_neighbors=8, poly_type='chebyshev',\n",
    "                       K=8, Fout=64, activation='relu', use_bn=True, \n",
    "                       kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x1)\n",
    "        x1 = hpf.HealpyPseudoConv(p=1, Fout=64, activation='relu', \n",
    "                                  initializer='he_normal', kernel_regularizer=self.l2(weight_decay), \n",
    "                                  nside=nside_out1, indices=indices_out1)(x1)\n",
    "        x1 = tf.keras.layers.SpatialDropout1D(sdrate)(x1)\n",
    "        x1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                epsilon=0.001, center=False, \n",
    "                                                scale=False)(x1) \n",
    "\n",
    "        \n",
    "        x2 = self.Conv(nside=nside_out1, indices=indices_out1, n_neighbors=20, poly_type='chebyshev',\n",
    "                       K=12, Fout=64, activation='relu', use_bn=True, \n",
    "                       kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x)\n",
    "        x2 = hpf.HealpyPseudoConv(p=1, Fout=64, activation='relu', \n",
    "                                  initializer='he_normal', kernel_regularizer=self.l2(weight_decay), \n",
    "                                  nside=nside_out1, indices=indices_out1)(x2)\n",
    "        x2 = tf.keras.layers.SpatialDropout1D(sdrate)(x2)\n",
    "        x2 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                epsilon=0.001, center=False, \n",
    "                                                scale=False)(x2) \n",
    "\n",
    "        \n",
    "        x3 = tf.keras.layers.Concatenate(axis=-1)([x1,x2])\n",
    "        \n",
    "        xres = hpf.HealpyPool(nside=nside_out1, indices = indices_out1, p=1, pool_type='AVG')(x)\n",
    "        xres = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                  epsilon=0.001, center=False, \n",
    "                                                  scale=False)(xres)        \n",
    "\n",
    "        nside_out2 = hpf.HealpyPool(nside=nside_out1, indices = indices_out1, \n",
    "                                    p=1, pool_type='AVG').nside_out\n",
    "        indices_out2 = hpf.HealpyPool(nside=nside_out1, indices = indices_out1, \n",
    "                                      p=1, pool_type='AVG').indices_out\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x3,xres])\n",
    "        \n",
    "        x1 = self.Conv(nside=nside_out2, indices=indices_out2, n_neighbors=8, poly_type='chebyshev',\n",
    "                      K=8, Fout=256, activation='relu', use_bn=True, \n",
    "                      kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x)\n",
    "        x = self.Conv(nside=nside_out2, indices=indices_out2, n_neighbors=20, poly_type='chebyshev',\n",
    "                      K=12, Fout=256, activation='relu', use_bn=True, \n",
    "                      kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x1)\n",
    "        \n",
    "        x1 = self.Conv(nside=nside_out2, indices=indices_out2, n_neighbors=8, poly_type='chebyshev',\n",
    "                      K=4, Fout=256, activation='relu', use_bn=True, \n",
    "                      kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x)\n",
    "        x1 = self.Conv(nside=nside_out2, indices=indices_out2, n_neighbors=20, poly_type='chebyshev',\n",
    "                      K=8, Fout=256, activation='relu', use_bn=True, \n",
    "                      kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x1)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x,x1])\n",
    "        \n",
    "        for i in range(2):\n",
    "            x1 = self.SeparableConv(nside=nside_out2,\n",
    "                                    indices=indices_out2,\n",
    "                                    n_neighbors=8,\n",
    "                                    poly_type='chebyshev',\n",
    "                                    K=6,\n",
    "                                    Fout=256,\n",
    "                                    depth_multiplier=2,\n",
    "                                    pointwise_initializer='he_normal',\n",
    "                                    depthwise_initializer='he_normal',\n",
    "                                    pointwise_regularizer=self.l2(weight_decay),\n",
    "                                    depthwise_regularizer=self.l2(weight_decay))(x)\n",
    "            x1 = tf.keras.layers.ReLU()(x1)\n",
    "            x1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                    epsilon=0.001, center=False, \n",
    "                                                    scale=False)(x1)\n",
    "\n",
    "            \n",
    "            x1 = self.SeparableConv(nside=nside_out2,\n",
    "                                    indices=indices_out2,\n",
    "                                    n_neighbors=20,\n",
    "                                    poly_type='chebyshev',\n",
    "                                    K=10,\n",
    "                                    Fout=256,\n",
    "                                    depth_multiplier=2,\n",
    "                                    pointwise_initializer='he_normal',\n",
    "                                    depthwise_initializer='he_normal',\n",
    "                                    pointwise_regularizer=self.l2(weight_decay),\n",
    "                                    depthwise_regularizer=self.l2(weight_decay))(x1)\n",
    "            x1 = tf.keras.layers.ReLU()(x1)\n",
    "            x1 = tf.keras.layers.SpatialDropout1D(sdrate)(x1)            \n",
    "            x1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                    epsilon=0.001, center=False, \n",
    "                                                    scale=False)(x1)         \n",
    "        \n",
    "            x = tf.keras.layers.Add()([x, x1])\n",
    "        \n",
    "        x = self.DepthwiseConv(nside=nside_out2, indices=indices_out2, n_neighbors=20, poly_type='chebyshev',\n",
    "                      K=8, depth_multiplier=2, activation='relu', use_bn=True, \n",
    "                      kernel_initializer='he_normal', kernel_regularizer=self.l2(weight_decay))(x)\n",
    "        x = hpf.HealpyPool(nside=nside_out2, indices=indices_out2, p=1, pool_type='AVG')(x)\n",
    "        x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                               epsilon=0.001, center=False, \n",
    "                                               scale=False)(x)\n",
    "        x = tf.keras.layers.SpatialDropout1D(sdrate)(x)\n",
    "        \n",
    "        nside_out3 = hpf.HealpyPool(nside=nside_out2, indices=indices_out2, p=1, pool_type='AVG').nside_out\n",
    "        indices_out3 = hpf.HealpyPool(nside=nside_out2, indices=indices_out2, p=1, pool_type='AVG').indices_out        \n",
    "        \n",
    "        for i in range(2):\n",
    "            x1 = self.SeparableConv(nside=nside_out3,\n",
    "                                    indices=indices_out3,\n",
    "                                    n_neighbors=20,\n",
    "                                    poly_type='chebyshev',\n",
    "                                    K=4,\n",
    "                                    Fout=512,\n",
    "                                    depth_multiplier=1,\n",
    "                                    pointwise_initializer='he_normal',\n",
    "                                    depthwise_initializer='he_normal',\n",
    "                                    pointwise_regularizer=self.l2(weight_decay),\n",
    "                                    depthwise_regularizer=self.l2(weight_decay))(x)\n",
    "            x1 = tf.keras.layers.ReLU()(x1)            \n",
    "            x1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                    epsilon=0.001, center=False, \n",
    "                                                    scale=False)(x1)\n",
    "\n",
    "            \n",
    "            x1 = self.SeparableConv(nside=nside_out3,\n",
    "                                    indices=indices_out3,\n",
    "                                    n_neighbors=8,\n",
    "                                    poly_type='chebyshev',\n",
    "                                    K=8,\n",
    "                                    Fout=512,\n",
    "                                    depth_multiplier=1,\n",
    "                                    pointwise_initializer='he_normal',\n",
    "                                    depthwise_initializer='he_normal',\n",
    "                                    pointwise_regularizer=self.l2(weight_decay),\n",
    "                                    depthwise_regularizer=self.l2(weight_decay))(x1)\n",
    "            x1 = tf.keras.layers.ReLU()(x1)   \n",
    "            x1 = tf.keras.layers.SpatialDropout1D(sdrate)(x1)            \n",
    "            x1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                    epsilon=0.001, center=False, \n",
    "                                                    scale=False)(x1)\n",
    "         \n",
    "        \n",
    "            x = tf.keras.layers.Add()([x, x1])        \n",
    "        \n",
    "        x = self.Conv(nside=nside_out3, indices=indices_out3, n_neighbors=8, poly_type='chebyshev',\n",
    "                      K=8, Fout=768, activation='relu', use_bn=True, \n",
    "                      kernel_regularizer=self.l2(weight_decay))(x)\n",
    "        x = hpf.HealpyPool(nside=nside_out3, indices=indices_out3, p=1, pool_type='AVG')(x)\n",
    "        x = tf.keras.layers.SpatialDropout1D(sdrate)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                               epsilon=0.001, center=False, \n",
    "                                               scale=False)(x)\n",
    "        \n",
    "        nside_out4 = hpf.HealpyPool(nside=nside_out3, indices=indices_out3, p=1, pool_type='AVG').nside_out\n",
    "        indices_out4 = hpf.HealpyPool(nside=nside_out3, indices=indices_out3, p=1, pool_type='AVG').indices_out\n",
    "        \n",
    "        for i in range(2):\n",
    "            x1 = self.SeparableConv(nside=nside_out4,\n",
    "                                    indices=indices_out4,\n",
    "                                    n_neighbors=8,\n",
    "                                    poly_type='chebyshev',\n",
    "                                    K=8,\n",
    "                                    Fout=768,\n",
    "                                    depth_multiplier=1,\n",
    "                                    pointwise_initializer='he_normal',\n",
    "                                    depthwise_initializer='he_normal',\n",
    "                                    pointwise_regularizer=self.l2(weight_decay),\n",
    "                                    depthwise_regularizer=self.l2(weight_decay))(x)\n",
    "            x1 = tf.keras.layers.ReLU()(x1)\n",
    "            x1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                    epsilon=0.001, center=False, \n",
    "                                                    scale=False)(x1)\n",
    "\n",
    "            \n",
    "            x1 = self.SeparableConv(nside=nside_out4,\n",
    "                                    indices=indices_out4,\n",
    "                                    n_neighbors=8,\n",
    "                                    poly_type='chebyshev',\n",
    "                                    K=8,\n",
    "                                    Fout=768,\n",
    "                                    depth_multiplier=1,\n",
    "                                    pointwise_initializer='he_normal',\n",
    "                                    depthwise_initializer='he_normal',\n",
    "                                    pointwise_regularizer=self.l2(weight_decay),\n",
    "                                    depthwise_regularizer=self.l2(weight_decay))(x1)\n",
    "            x1 = tf.keras.layers.ReLU()(x1) \n",
    "            x1 = tf.keras.layers.SpatialDropout1D(sdrate)(x1)            \n",
    "            x1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                                    epsilon=0.001, center=False, \n",
    "                                                    scale=False)(x1)\n",
    "           \n",
    "        \n",
    "            x = tf.keras.layers.Add()([x, x1])     \n",
    "        \n",
    "        x = self.SeparableConv(nside=nside_out4,\n",
    "                               indices=indices_out4,\n",
    "                               n_neighbors=8,\n",
    "                               poly_type='chebyshev',\n",
    "                               K=8,\n",
    "                               Fout=768,\n",
    "                               depth_multiplier=1,\n",
    "                               pointwise_initializer='he_normal',\n",
    "                               depthwise_initializer='he_normal',\n",
    "                               pointwise_regularizer=self.l2(weight_decay),\n",
    "                               depthwise_regularizer=self.l2(weight_decay))(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, \n",
    "                                               epsilon=0.001, center=False, \n",
    "                                               scale=False)(x)\n",
    "        \n",
    "        if include_top == True:\n",
    "            outputs = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "            outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(outputs)\n",
    "                               \n",
    "        \n",
    "        return Model(inputs = inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "print('Loading data...')\n",
    "a_lm_triv = np.load('data/realizations_L_infty_lmax_250_num_1000.npy').astype(np.complex128)\n",
    "a_lm_torus1400 = np.load('data/realizations_L_1400_lmax_250_num_1000.npy').astype(np.complex128)\n",
    "a_lm_torus2800 = np.load('data/realizations_L_2800_lmax_250_num_1000.npy').astype(np.complex128)\n",
    "print('Data loading complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input indices and masking:\n",
    "print('Preparing the mask and calculating relevant map indices')\n",
    "nside = 128\n",
    "npix = hp.nside2npix(nside=nside)\n",
    "indices = np.arange(npix)\n",
    "mask=hp.read_map('data/masks/COM_Mask_CMB-common-Mask-Int_2048_R3.fits')\n",
    "print('Mask preparation done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unmasked pixels:\n",
    "unmasked_pix = pt.get_indices(mask=mask, nside=nside, target_nside=nside)\n",
    "#aggresive masking: (extend the mask)\n",
    "worst_case_pix = pt.get_indices(mask=mask, nside=nside, target_nside=8)\n",
    "#adaptive masking: (reduce the indices minimally only if pooling is to occur)\n",
    "adaptive_case_pix = pt.get_indices(mask=mask, nside=nside, target_nside=nside//2) #//2 is not necessary.\n",
    "print('Relevant map indices are calculated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting some strategy will still work on single GPU.\n",
    "print('Defining TensorFlow distribution strategy.')\n",
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.NcclAllReduce(num_packs=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating TensorFlow datasets.')\n",
    "BATCH_SIZE_PER_REPLICA = 5\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "#create datasets:\n",
    "train_data, test_data, x_eval, y_eval, x_alm_train, y_train = pt.create_dataset(a_lm_torus1400[0:100], \n",
    "                                                                         a_lm_torus2800[0:100], \n",
    "                                                                         a_lm_triv[0:100], \n",
    "                                                                         relevant_pix=adaptive_case_pix,\n",
    "                                                                         global_batch_size=GLOBAL_BATCH_SIZE,\n",
    "                                                                         trainperc=0.8,\n",
    "                                                                         evalperc=0.05,\n",
    "                                                                         strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset creation completed.')\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "print('Creating the model.')\n",
    "with strategy.scope():\n",
    "    GCN = GCNHealpy(nside=nside, \n",
    "                      indices=adaptive_case_pix, \n",
    "                      channels=1,\n",
    "                      use_polyK=False)\n",
    "    model = GCNHealpy(nside=nside, \n",
    "                      indices=adaptive_case_pix, \n",
    "                      channels=1,\n",
    "                      use_polyK=False).model(weight_decay=1e-4, \n",
    "                                        sdrate=0.05, \n",
    "                                        include_top=True,\n",
    "                                        num_classes=3)\n",
    "print('Model creation complete.')\n",
    "\n",
    "model.summary(110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "#checkpoint_path = \"runs_2/training_3_class/SGDopt_xception_v3_L_precalc_adaptive_mask/cp-{epoch:04d}.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "#checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "def lr_decay(lr_init, epoch, num_batches, decay=0.998):\n",
    "    steps = epoch * BUFFER_SIZE//GLOBAL_BATCH_SIZE + num_batches\n",
    "    if epoch < 20:\n",
    "        return lr_init\n",
    "    else:\n",
    "        return lr_init* (decay)**(-20*BUFFER_SIZE//GLOBAL_BATCH_SIZE)*(decay)**(steps)\n",
    "    \n",
    "with strategy.scope():\n",
    "  # Set reduction to `none` so we can do the reduction afterwards and divide by\n",
    "  # global batch size.\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3,  #1e-3 with mom = 0.8 and decay = 0.998 is very stable\n",
    "                                        momentum = 0.8,\n",
    "                                        nesterov=False)     #but seems to stagnate (or run out of data)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "\n",
    "\n",
    "def train_step(inputs):\n",
    "    samples, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(samples, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss \n",
    "\n",
    "def test_step(inputs):\n",
    "    samples, labels = inputs\n",
    "\n",
    "    predictions = model(samples, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss.update_state(t_loss)\n",
    "    test_accuracy.update_state(labels, predictions)\n",
    "\n",
    "# `run` replicates the provided computation and runs it\n",
    "# with the distributed input.\n",
    "@tf.function()\n",
    "def distributed_train_step(dataset_inputs):\n",
    "    per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None)\n",
    "\n",
    "@tf.function()\n",
    "def distributed_test_step(dataset_inputs):\n",
    "    return strategy.run(test_step, args=(dataset_inputs,))\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "logs = {}\n",
    "\n",
    "BUFFER_SIZE = len(x_alm_train)\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "AUG_EPOCH = 50 #at the end of 50th, 100th etc epochs, we will rotate the training data randomly\n",
    "print(\"------------------------------------------------------\")\n",
    "print(f\"Starting Training, Epochs:{EPOCHS}, Augmentation Epochs:{EPOCHS//AUG_EPOCH}\")\n",
    "print(\"------------------------------------------------------\")\n",
    "for epoch in range(EPOCHS):\n",
    "    ### TRAIN LOOP ###\n",
    "    print(f\"Starting with Epoch {epoch + 1}/{EPOCHS}\", flush=True)\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0    \n",
    "    with tqdm(train_data, total=BUFFER_SIZE//GLOBAL_BATCH_SIZE) as pbar:\n",
    "        for x in pbar:\n",
    "            optimizer.learning_rate = lr_decay(5e-4, epoch, num_batches, 0.9995)\n",
    "            pbar.set_description(f\"Epoch {epoch +1}/{EPOCHS}\", refresh=True)\n",
    "            total_loss += distributed_train_step(x)\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'train_loss': total_loss.numpy()/num_batches,\n",
    "                              'learning_rate': optimizer.learning_rate.numpy()}, refresh=True)\n",
    "            \n",
    "        train_loss = total_loss / num_batches\n",
    "\n",
    "    ### TEST LOOP ###\n",
    "        for x in test_data:\n",
    "            distributed_test_step(x)\n",
    "\n",
    "    template = (\"Epoch {}/{}, Training Loss: {:.5g}, Training Accuracy: {:.5g}, Test Loss: {:.5g}, \"\n",
    "                \"Test Accuracy: {:.5g}\")\n",
    "    print (template.format(epoch+1, EPOCHS,train_loss.numpy(),\n",
    "                             train_accuracy.result().numpy(), test_loss.result().numpy(),\n",
    "                             test_accuracy.result().numpy()))\n",
    "    train_loss.append(train_loss)\n",
    "    train_accuracy.append(train_accuracy.result())\n",
    "    test_loss.append(test_loss.result())\n",
    "    test_accuracy.append(test_accuracy.result())\n",
    "    \n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    ### DATASET ROTATION ###\n",
    "    if epoch > 0:\n",
    "        if (epoch)%AUG_EPOCH == 0:\n",
    "            print(f\"Augmentation Epoch {(epoch+1)//AUG_EPOCH}/{EPOCHS//AUG_EPOCH} \")\n",
    "            print('Rotating training dataset...')\n",
    "            train_data = pt.rotate_train_data(alm=x_alm_train, y_train=y_train, \n",
    "                                                  relevant_pix=adaptive_case_pix, \n",
    "                                                  global_batch_size=GLOBAL_BATCH_SIZE, \n",
    "                                                  strategy=strategy)\n",
    "            print('Rotation complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1,EPOCHS+1)\n",
    "fig, axes = plt.subplots(2, figsize=(10, 10), sharex=True)\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(epochs, train_loss_xception_v2, label = 'Training')\n",
    "axes[0].plot(epochs, test_loss_xception_v2, '--', label = 'Validation')\n",
    "axes[0].grid(visible=True, axis='both')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Training Metrics (Adaptive Masking)')\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(epochs, train_accuracy_xception_v2, label = 'Training')\n",
    "axes[1].plot(epochs, test_accuracy_xception_v2, '--', label = 'Validation')\n",
    "axes[1].grid(visible=True, axis='both')\n",
    "#axes[1].set_yscale('log')\n",
    "axes[1].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
