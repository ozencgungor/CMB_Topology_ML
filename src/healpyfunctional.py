import numpy as np
import matplotlib.pyplot as plt

from scipy import sparse
from scipy.sparse.linalg import eigsh

import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Layer

import healpy as hp
from pygsp.graphs import SphereHealpix
from pygsp import filters

from deepsphere import utils

from tqdm import tqdm

###
#TODO: play with tf.einsum and tensordot
#implement passing P(L) directly.
###

"""
Defining healpy convolution layers as layers that are callable on tensors to aid in keras functional API.
"""

def rescale_L(L, lmax=2, scale=1):
    """Rescale the Laplacian eigenvalues in [-scale,scale]."""
    M, M = L.shape
    I = sparse.identity(M, format='csr', dtype=L.dtype)
    L *= 2 * scale / lmax
    L -= I
    return L


def get_sparse_L(nside, indices, n_neighbors):
    """
    Returns the healpix graph laplacian in tf.sparse format
    args:
    nside: nside of the maps
    indices: indices of the maps
    n_neighbors: number of neighbors of each pixel of the graph
    """
    sphere = SphereHealpix(subdivisions=nside, indexes=indices, nest=True, 
                               k=n_neighbors, lap_type='normalized')
    L = sphere.L
    L = sparse.csr_matrix(L)
    lmax = 1.02 * eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]
    L = rescale_L(L, lmax=lmax, scale=0.75)
    L = L.tocoo()
    indices_L = np.column_stack((L.row, L.col))
    L = tf.SparseTensor(indices_L, L.data, L.shape)
    sparse_L = tf.sparse.reorder(L)
    return sparse_L

def transform_L(L):
    L = sparse.csr_matrix(L)
    lmax = 1.02 * eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]
    L = rescale_L(L, lmax=lmax, scale=0.75)
    L = L.tocoo()
    indices_L = np.column_stack((L.row, L.col))
    L = tf.SparseTensor(indices_L, L.data, L.shape)
    sparse_L = tf.sparse.reorder(L)
    return sparse_L

def construct_tf_sparse(L):
    L_tf = L.tocoo()
    indices_L_tf = np.column_stack((L_tf.row, L_tf.col))
    L_tf = tf.SparseTensor(indices_L_tf, L_tf.data, L_tf.shape)
    L_tf = tf.sparse.reorder(L_tf)
    return L_tf

def reduce_indices(nside_in, nside_out, indices_in):
    """
    Minimally reduces a set of indices such that it can be reduced to nside_out in a healpy fashion. Four pixels reduce
    naturally to a higher order superpixel. The pixel indices returned by this function will be pixels that do not have
    any masked pixel belonging to the same superpixel as the pixel itself.
    Indices must be in NEST ordering.
    :param indices: 1d array of integer pixel ids
    :param nside_in: nside of the input
    :param nside_out: nside of the output
    :param nest: indices are ordered in the "NEST" ordering scheme
    :return: returns a set of indices in the same ordering as the input.
    """
    
    ordering = "NEST"

    # get the map to reduce
    m_in = np.ones(hp.nside2npix(nside_in))
    m_in[indices_in] = 0.0

    # reduce
    m_in = hp.ud_grade(map_in=m_in, nside_out=nside_out, order_in=ordering, order_out=ordering)

    # expand
    m_in = hp.ud_grade(map_in=m_in, nside_out=nside_in, order_in=ordering, order_out=ordering)

    # get the new indices
    return np.arange(hp.nside2npix(nside_in))[m_in < 1e-12] 

@tf.function()
def reduce_indices_v2(indices, p):
    """
    Minimally reduces a set of indices such that it can be reduced to nside_out in a healpy fashion. Four pixels reduce
    naturally to a higher order superpixel. The pixel indices returned by this function will be pixels that do not have
    any masked pixel belonging to the same superpixel as the pixel itself. Could be faster than the healpy version.
    Does not require information about the current nside.
    :param indices: 1d array of integer pixel indices. tf.tensor or np.ndarray
    :param p: integer. reduction factor (nside -> nside/(2**p))
    :returns: tf.tensor of new indices.
    """
    indices = tf.cast(tf.convert_to_tensor(indices), dtype=tf.int32)
    new_indices = tf.fill(indices.shape, -1)
    for i in tf.range(indices.shape[0]-((4**p)-1)):
        if indices[i]%(4**p)==0:
            if tf.experimental.numpy.array_equal(indices[i:i + 4**p], tf.range(indices[i], indices[i] + 4**p, dtype=tf.int32)):
                new_indices = tf.tensor_scatter_nd_update(new_indices, tf.reshape(tf.range(i,i + 4**p),[4**p,1]), indices[i:i + 4**p])
    return tf.squeeze(tf.gather(new_indices, tf.where(new_indices>-1)))

def transformed_indices(nside_in, nside_out, indices):
    """
    utility function to get new indices after a change in n_side
    
    Arguments:
    nside_in: n_side of the input map, integer of form 2**p
    nside_out: n_side of the output map, integer of form 2**p
    indices: set of healpy map indices, if mask is applied, indices should
             be generated by deepsphere.extend_indices. should be 
             in nest ordering.
             
    Returns:
    transformed_indices: the set of indices with n_side out
                         given n_side in and the set of indices.
    """
    mask_in = np.zeros(hp.nside2npix(nside_in))
    mask_in[indices] = 1.0
    mask_out = hp.ud_grade(map_in=mask_in, nside_out=nside_out, order_in="NEST", order_out="NEST")
    transformed_indices = np.arange(hp.nside2npix(nside_out))[mask_out > 1e-12]
    return transformed_indices

@tf.function
def transformed_indices_v2(indices, p, reduce=True):
    """
    Utility function to get new indices after a change in nside
    :param indices: ndarray or tf.tensor. current indices to be transformed to new nside
    :param p: int > 0. reduction(boost) factor, nside -> nside/(2**p) or nside*(2**p)
    :param reduce: bool. whether nside is being reduced or increased. if true, will 
                   calculate the new indices for nside -> nside/(2**p), if false, will
                   calculate the new indices for nside -> nside*(2**p). defaults to true.
    returns: a tf.tensor. indices for the new nside.
    """
    indices = tf.cast(tf.convert_to_tensor(indices), dtype=tf.int32)
    if reduce:
        super_pix = indices%(4**p)
        new_indices = tf.squeeze(tf.gather(indices, tf.where(super_pix == 0)))//4
    else:
        new_indices = tf.repeat((4**p)*indices, (4**p))
        new_indices = new_indices + tf.range((4**p)*indices.shape[0],dtype=tf.int32)%(4**p)
    return new_indices


class HealpyChebyshevConv(Layer):
    """
    A graph convolutional layer using the Chebyshev approximation
    """

    def __init__(self, nside, indices, n_neighbors, K, K0 = 0, Fout=None, initializer=None, activation=None, 
                 use_bias=False, use_bn=False, kernel_regularizer=None, L=None, **kwargs):
        """
        Initializes the graph convolutional layer, assuming the input has dimension (B, M, F)
        :param nside: nside of the input map
        :param indices: indices of the input map
        :param n_neighbors: n_neighbors of the graph laplacian
        :param K0: if specified, will reduce the kernel size to K-K0 and will only consider the last K-K0 
                   legendre polynomials. K0 < K
        :param K: Order of the polynomial to use
        :param Fout: Number of features (channels) of the output, default to number of input channels
        :param initializer: optional. initializer to use for weight initialization, defaults to 'he_normal'
        :param activation: the activation function to use after the layer, defaults to linear
        :param use_bias: Use learnable bias weights
        :param use_bn: Apply batch norm before adding the bias
        :param kernel_regularizer: kernel regularizer
        :param L: optional. graph Laplacian L of shape indices*incides in tf.sparse format. if included it will be used 
                  directlyinstead of calculating it from nside and indices. either L or (nside, indices, n_neighbors) 
                  need to be given. If L is a numpy array, use transform_L(L) to make it a tf.sparse tensor.
        :param kwargs: additional keyword arguments passed on to add_weight
        """

        # This is necessary for every Layer
        super(HealpyChebyshevConv, self).__init__(**kwargs)

        # save necessary parameters
        self.nside = nside
        self.indices = indices
        self.n_neighbors = n_neighbors
        self.K = K
        self.K0 = K0
        self.Fout = Fout
        self.use_bias = use_bias
        self.use_bn = use_bn
        self.kernel_regularizer = kernel_regularizer
        if self.use_bn:
            self.bn = tf.keras.layers.BatchNormalization(axis=-1, 
                                                         momentum=0.9, 
                                                         epsilon=1e-5, 
                                                         center=False, 
                                                         scale=False)

        if initializer is None:
            self.initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, initializer):
            self.initializer = getattr(tf.keras.initializers, initializer)
        else:
            raise ValueError(f"Could not find initializer <{initializer}> in tf.keras.initializers...")
        
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        
        self.L = L
        
        #if above works, delete till self.kwargs
        if self.L:
            self.sparse_L = L
        else:
            self.sparse_L = get_sparse_L(nside=self.nside, indices=self.indices, n_neighbors = self.n_neighbors)
            
        self.kwargs = kwargs
        
    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        :return: the kernel variable to train
        """

        # get the input shape
        Fin = int(input_shape[-1])

        # get Fout if necessary
        if self.Fout is None:
            Fout = Fin
        else:
            Fout = self.Fout

        self.kernel = self.add_weight("kernel", 
                                      shape=[Fin*(self.K-self.K0), Fout],
                                      trainable=True,
                                      initializer=self.initializer, 
                                      regularizer=self.kernel_regularizer,
                                      **self.kwargs)

        if self.use_bias:
            self.bias = self.add_weight("bias", 
                                        shape=[1, 1, Fout],
                                        trainable=True)

        # we cast the sparse L to the current backend type
        if tf.keras.backend.floatx() == 'float16':
            self.sparse_L = tf.cast(self.sparse_L, tf.float16)
        if tf.keras.backend.floatx() == 'float32':
            self.sparse_L = tf.cast(self.sparse_L, tf.float32)
        if tf.keras.backend.floatx() == 'float64':
            self.sparse_L = tf.cast(self.sparse_L, tf.float64)

    def call(self, input_tensor, training=True, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param training: wheter we are training or not
        :param kwargs: further keyword arguments
        :return: the output of the layer
        """

        # shapes, this fun is necessary since sparse_matmul_dense in TF only supports
        # the multiplication of 2d matrices, therefore one has to do some weird reshaping
        # this is not strictly necessary but leads to a huge performance gain...
        # See: https://arxiv.org/pdf/1903.11409.pdf
        N, M, Fin = input_tensor.get_shape()
        M, Fin = int(M), int(Fin)

        # get Fout if necessary
        if self.Fout is None:
            Fout = Fin
        else:
            Fout = self.Fout
        #input tensor N x M x F
        # Transform to Legendre basis, x0 -> P_0 -> P_0(L) = I
        x0 = tf.transpose(input_tensor, perm=[1, 2, 0])  # M x Fin x N
        x0 = tf.reshape(x0, [M, -1])  # M x Fin*N

        # list for stacking
        stack = [x0]

        if self.K > 1:
            x1 = tf.sparse.sparse_dense_matmul(self.sparse_L, x0) #P_1(L) = L
            stack.append(x1)
        for k in range(2, self.K): #legendre polynomial recursion rule T_k = 2 * x * T_{k-1} - T_{k-2}
            x2 = 2 * tf.sparse.sparse_dense_matmul(self.sparse_L, x1) - x0  # M x Fin*N
            stack.append(x2)
            x0, x1 = x1, x2
        x = tf.stack(stack, axis=0)[self.K0:] #K x M x Fin*N
        x = tf.reshape(x, [(self.K-self.K0), M, Fin, -1])  # K x M x Fin x N
        x = tf.transpose(x, perm=[3, 1, 2, 0])  # N x M x Fin x K
        x = tf.reshape(x, [-1, Fin * (self.K-self.K0)])  # N*M x Fin*K
        # Filter: Fin*Fout filters of order K, i.e. one filterbank per output feature.
        x = tf.matmul(x, self.kernel)  # N*M x Fout
        x = tf.reshape(x, [-1, M, Fout])  # N x M x Fout

        if self.use_bias:
            x = tf.add(x, self.bias)

        if self.activation is not None:
            x = self.activation(x)

        if self.use_bn:
            x = self.bn(x, training=training)
        
        return x
    

    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  #"n_neighbors":self.n_neighbors,
                  "K":self.K,
                  "K0":self.K0,
                  "Fout":self.Fout,
                  "initializer":self.initializer,
                  "activation":self.activation,
                  "use_bias":self.use_bias,
                  "use_bn":self.use_bn,
                  "kernel_regularizer":self.kernel_regularizer,
                  "poly_type":"legendre",
                  #"L":self.L,
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

class HealpyLegendreConv(Layer):
    """
    A graph convolutional layer using the Legendre approximation
    """

    def __init__(self, nside, indices, n_neighbors, K, K0 = 0, Fout=None, initializer=None, activation=None, 
                 use_bias=False, use_bn=False, kernel_regularizer=None, L=None, **kwargs):
        """
        Initializes the graph convolutional layer, assuming the input has dimension (B, M, F)
        :param nside: nside of the input map
        :param indices: indices of the input map
        :param n_neighbors: n_neighbors of the graph laplacian
        :param K0: if specified, will reduce the kernel size to K-K0 and will only consider the last K-K0 
                   legendre polynomials. K0 < K
        :param K: Order of the polynomial to use
        :param Fout: Number of features (channels) of the output, default to number of input channels
        :param initializer: optional. initializer to use for weight initialization, defaults to 'he_normal'
        :param activation: the activation function to use after the layer, defaults to linear
        :param use_bias: Use learnable bias weights
        :param use_bn: Apply batch norm before adding the bias
        :param kernel_regularizer: kernel regularizer
        :param L: optional. graph Laplacian L of shape indices*incides in tf.sparse format. if included it will be used 
                  directlyinstead of calculating it from nside and indices. either L or (nside, indices, n_neighbors) 
                  need to be given. If L is a numpy array, use transform_L(L) to make it a tf.sparse tensor.
        :param kwargs: additional keyword arguments passed on to add_weight
        """

        # This is necessary for every Layer
        super(HealpyLegendreConv, self).__init__(**kwargs)

        # save necessary parameters
        self.nside = nside
        self.indices = indices
        self.n_neighbors = n_neighbors
        self.K = K
        self.K0 = K0
        self.Fout = Fout
        self.use_bias = use_bias
        self.use_bn = use_bn
        self.kernel_regularizer = kernel_regularizer
        if self.use_bn:
            self.bn = tf.keras.layers.BatchNormalization(axis=-1, 
                                                         momentum=0.9, 
                                                         epsilon=1e-5, 
                                                         center=False, 
                                                         scale=False)

        if initializer is None:
            self.initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, initializer):
            self.initializer = getattr(tf.keras.initializers, initializer)
        else:
            raise ValueError(f"Could not find initializer <{initializer}> in tf.keras.initializers...")
        
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        
        self.L = L
        
        """ #this is for testing, test on cheby first then implement here.
        if self.L is None:
            if self.nside or self.indices or self.n_neighbors is None:
                raise ValueError("Either the graph laplacian L or (nside, indices, n_neighbors have to be given!")
            else:
                sphere = SphereHealpix(subdivisions=nside, indexes=indices, nest=True, 
                                       k=n_neighbors, lap_type='normalized')
                L = sphere.L
                L = sparse.csr_matrix(L)
                lmax = 1.02 * eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]
                L = utils.rescale_L(L, lmax=lmax, scale=0.75)
                L = L.tocoo()
                indices_L = np.column_stack((L.row, L.col))
                L = tf.SparseTensor(indices_L, L.data, L.shape)
                self.sparse_L = tf.sparse.reorder(L)
        else:
            self.sparse_L = L        
        """    
        
        #if above works, delete till self.kwargs
        if self.L:
            self.sparse_L = L
        else:
            self.sparse_L = get_sparse_L(nside=self.nside, indices=self.indices, n_neighbors = self.n_neighbors)
            
        self.kwargs = kwargs
        
    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        :return: the kernel variable to train
        """

        # get the input shape
        Fin = int(input_shape[-1])

        # get Fout if necessary
        if self.Fout is None:
            Fout = Fin
        else:
            Fout = self.Fout

        self.kernel = self.add_weight("kernel", 
                                      shape=[Fin*(self.K-self.K0), Fout],
                                      trainable=True,
                                      initializer=self.initializer, 
                                      regularizer=self.kernel_regularizer,
                                      **self.kwargs)

        if self.use_bias:
            self.bias = self.add_weight("bias", 
                                        shape=[1, 1, Fout],
                                        trainable=True)

        # we cast the sparse L to the current backend type
        if tf.keras.backend.floatx() == 'float16':
            self.sparse_L = tf.cast(self.sparse_L, tf.float16)
        if tf.keras.backend.floatx() == 'float32':
            self.sparse_L = tf.cast(self.sparse_L, tf.float32)
        if tf.keras.backend.floatx() == 'float64':
            self.sparse_L = tf.cast(self.sparse_L, tf.float64)

    def call(self, input_tensor, training=True, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param training: wheter we are training or not
        :param kwargs: further keyword arguments
        :return: the output of the layer
        """

        # shapes, this fun is necessary since sparse_matmul_dense in TF only supports
        # the multiplication of 2d matrices, therefore one has to do some weird reshaping
        # this is not strictly necessary but leads to a huge performance gain...
        # See: https://arxiv.org/pdf/1903.11409.pdf
        N, M, Fin = input_tensor.get_shape()
        M, Fin = int(M), int(Fin)

        # get Fout if necessary
        if self.Fout is None:
            Fout = Fin
        else:
            Fout = self.Fout
        #input tensor N x M x F
        # Transform to Legendre basis, x0 -> P_0 -> P_0(L) = I
        x0 = tf.transpose(input_tensor, perm=[1, 2, 0])  # M x Fin x N
        x0 = tf.reshape(x0, [M, -1])  # M x Fin*N

        # list for stacking
        stack = [x0]

        if self.K > 1:
            x1 = tf.sparse.sparse_dense_matmul(self.sparse_L, x0) #P_1(L) = L
            stack.append(x1)
        for k in range(2, self.K): #legendre polynomial recursion rule P_k = (2*k-1)/k * x * P_{k-1} - (k-1)/k * P_{k-2}
            x2 = ((2*k-1)/k) * tf.sparse.sparse_dense_matmul(self.sparse_L, x1) - ((k-1)/k)*x0  # M x Fin*N
            stack.append(x2)
            x0, x1 = x1, x2
        x = tf.stack(stack, axis=0)[self.K0:] #K x M x Fin*N
        x = tf.reshape(x, [(self.K-self.K0), M, Fin, -1])  # K x M x Fin x N
        x = tf.transpose(x, perm=[3, 1, 2, 0])  # N x M x Fin x K
        x = tf.reshape(x, [-1, Fin * (self.K-self.K0)])  # N*M x Fin*K
        # Filter: Fin*Fout filters of order K, i.e. one filterbank per output feature.
        x = tf.matmul(x, self.kernel)  # N*M x Fout
        x = tf.reshape(x, [-1, M, Fout])  # N x M x Fout

        if self.use_bias:
            x = tf.add(x, self.bias)

        if self.activation is not None:
            x = self.activation(x)

        if self.use_bn:
            x = self.bn(x, training=training)

        return x
    

    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  #"n_neighbors":self.n_neighbors,
                  "K":self.K,
                  "K0":self.K0,
                  "Fout":self.Fout,
                  "initializer":self.initializer,
                  "activation":self.activation,
                  "use_bias":self.use_bias,
                  "use_bn":self.use_bn,
                  "kernel_regularizer":self.kernel_regularizer,
                  "poly_type":"legendre",
                  #"L":self.L,
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))


class HealpyDepthwiseLegendreConv(Layer):
    """
    A graph convolutional layer using the Legendre approximation
    """

    def __init__(self, nside, indices, n_neighbors, K, K0 = 0, depth_multiplier=1, initializer=None, activation=None, 
                 use_bias=False, use_bn=False, kernel_regularizer=None, L=None, **kwargs):
        """
        Initializes the graph convolutional layer, assuming the input has dimension (B, M, F)
        :param nside: nside of the input map
        :param indices: indices of the input map
        :param n_neighbors: n_neighbors of the graph laplacian
        :param K0: if specified, will reduce the kernel size to K-K0 and will only consider the last K-K0 
                   legendre polynomials. K0 < K
        :param K: Order of the polynomial to use
        :param depth_multiplier: Number of depthwise features (channels) of the output, defaults to 1
                      given an input tensor of shape (batch, num_pixels, channels), the output
                      will be of shape (batch, num_pixels, depth_multiplier*channels)
        :param initializer: initializer to use for weight initialisation
        :param activation: the activation function to use after the layer, defaults to linear
        :param use_bias: Use learnable bias weights
        :param use_bn: Apply batch norm before adding the bias
        :param kernel_regularizer: kernel regularizer
        :param L: Optional. graph Laplacian L of shape indices*incides. if included it will be used directly instead
                  of calculating it from nside and indices. either L or (nside, indices, n_neighbors) need to be
                  given.
        :param kwargs: additional keyword arguments passed on to add_weight
        """

        # This is necessary for every Layer
        super(HealpyDepthwiseLegendreConv, self).__init__(**kwargs)

        # save necessary parameters
        self.nside = nside
        self.indices = indices
        self.n_neighbors = n_neighbors
        self.K = K
        self.K0 = K0
        self.depth_multiplier = depth_multiplier
        self.use_bias = use_bias
        self.use_bn = use_bn
        self.kernel_regularizer = kernel_regularizer
        if self.use_bn:
            self.bn = tf.keras.layers.BatchNormalization(axis=-1, 
                                                         momentum=0.9, 
                                                         epsilon=1e-5, 
                                                         center=False, 
                                                         scale=False)
        
        if initializer is None:
            self.initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, initializer):
            self.initializer = getattr(tf.keras.initializers, initializer)
        else:
            raise ValueError(f"Could not find initializer <{initializer}> in tf.keras.initializers...")
        
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        
        self.L = L
        if self.L:
            self.sparse_L = L
        else:
            self.sparse_L = get_sparse_L(nside=self.nside, indices=self.indices, n_neighbors = self.n_neighbors)
        
        self.kwargs = kwargs
    
    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        :return: the kernel variable to train
        """

        # get the input shape
        Fin = int(input_shape[-1])

        self.kernel = self.add_weight("kernel", 
                                      shape=[Fin, self.depth_multiplier, (self.K-self.K0)],
                                      trainable=True,
                                      initializer=self.initializer, 
                                      regularizer=self.kernel_regularizer,
                                      **self.kwargs)

        if self.use_bias:
            self.bias = self.add_weight("bias", 
                                        shape=[1, 1, Fin*self.depth_multiplier],
                                        trainable=True
                                        )

        # we cast the sparse L to the current backend type
        if tf.keras.backend.floatx() == 'float16':
            self.sparse_L = tf.cast(self.sparse_L, tf.float16)
        if tf.keras.backend.floatx() == 'float32':
            self.sparse_L = tf.cast(self.sparse_L, tf.float32)
        if tf.keras.backend.floatx() == 'float64':
            self.sparse_L = tf.cast(self.sparse_L, tf.float64)

    def call(self, input_tensor, training=True, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param training: wheter we are training or not
        :param kwargs: further keyword arguments
        :return: the output of the layer
        """

        # shapes, this fun is necessary since sparse_matmul_dense in TF only supports
        # the multiplication of 2d matrices, therefore one has to do some weird reshaping
        # this is not strictly necessary but leads to a huge performance gain...
        # See: https://arxiv.org/pdf/1903.11409.pdf
        N, M, Fin = input_tensor.get_shape()
        M, Fin = int(M), int(Fin)


        # Transform to Legendre basis, x0 -> P_0 -> P_0(L) = I
        x0 = tf.transpose(input_tensor, perm=[1, 0, 2])  # M x N x Fin
        x0 = tf.reshape(x0, [M, -1])  # M x N*Fin

        # list for stacking
        stack = [x0]  #P_0(L) = 1
        if self.K > 1:
            x1 = tf.sparse.sparse_dense_matmul(self.sparse_L, x0) #P_1(L) = L
            stack.append(x1)
        for k in range(2, self.K): #legendre polynomial recursion rule P_k = (2*k-1)/k * x * P_{k-1} - (k-1)/k * P_{k-2}
            x2 = ((2*k-1)/k) * tf.sparse.sparse_dense_matmul(self.sparse_L, x1) - ((k-1)/k)*x0  # M x Fin*N
            stack.append(x2)
            x0, x1 = x1, x2
        
        #depthwise step
        x = tf.stack(stack, axis=0)[self.K0:] #(K-K0) x M x N*Fin
        x = tf.reshape(x, [(self.K-self.K0), M, -1, Fin])  # (K-K0) x M x N x Fin
        x = tf.transpose(x, perm=[3, 0, 2, 1]) # Fin x (K-K0) x N x M
        x = tf.reshape(x, [Fin, self.K-self.K0, -1]) # Fin x (K-K0) x N*M
        #depthwise kernel shape is Fin x depth_multiplier x (K-K0)
        #there are depth_multiplier many kernels of size K-K0 for each Fin
        x = tf.matmul(self.kernel, x)  # Fin x depth_multiplier x N*M  this uses tf.matmul batch multiplication rules
        x = tf.transpose(x, [2,0,1]) # N*M x Fin x depth_multiplier
        x = tf.reshape(x, [-1, M, Fin*self.depth_multiplier])

        if self.use_bias:
            x = tf.add(x, self.bias)

        if self.activation is not None:
            x = self.activation(x)

        if self.use_bn:
            x = self.bn(x, training=training)

        return x

    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  #"n_neighbors":self.n_neighbors,
                  "K":self.K,
                  "K0":self.K0,
                  "depth_multiplier":self.dFout,
                  "initializer":self.initializer,
                  "activation":self.activation,
                  "use_bias":self.use_bias,
                  "use_bn":self.use_bn,
                  "kernel_regularizer":self.kernel_regularizer,
                  "poly_type":"legendre",
                  #"L":self.L,
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

class HealpyDepthwiseChebyshevConv(Layer):
    """
    A graph convolutional layer using the Legendre approximation
    """

    def __init__(self, nside, indices, n_neighbors, K, K0 = 0, depth_multiplier=1, initializer=None, activation=None, 
                 use_bias=False, use_bn=False, kernel_regularizer=None, L=None, **kwargs):
        """
        Initializes the graph convolutional layer, assuming the input has dimension (B, M, F)
        :param nside: nside of the input map
        :param indices: indices of the input map
        :param n_neighbors: n_neighbors of the graph laplacian
        :param K0: if specified, will reduce the kernel size to K-K0 and will only consider the last K-K0 
                   legendre polynomials. K0 < K
        :param K: Order of the polynomial to use
        :param depth_multiplier: Number of depthwise features (channels) of the output, defaults to 1
                      given an input tensor of shape (batch, num_pixels, channels), the output
                      will be of shape (batch, num_pixels, depth_multiplier*channels)
        :param initializer: initializer to use for weight initialisation
        :param activation: the activation function to use after the layer, defaults to linear
        :param use_bias: Use learnable bias weights
        :param use_bn: Apply batch norm before adding the bias
        :param kernel_regularizer: kernel regularizer
        :param L: Optional. graph Laplacian L of shape indices*incides. if included it will be used directly instead
                  of calculating it from nside and indices. either L or (nside, indices, n_neighbors) need to be
                  given.
        :param kwargs: additional keyword arguments passed on to add_weight
        """

        # This is necessary for every Layer
        super(HealpyDepthwiseChebyshevConv, self).__init__(**kwargs)

        # save necessary parameters
        self.nside = nside
        self.indices = indices
        self.n_neighbors = n_neighbors
        self.K = K
        self.K0 = K0
        self.depth_multiplier = depth_multiplier
        self.use_bias = use_bias
        self.use_bn = use_bn
        self.kernel_regularizer = kernel_regularizer
        if self.use_bn:
            self.bn = tf.keras.layers.BatchNormalization(axis=-1, 
                                                         momentum=0.9, 
                                                         epsilon=1e-5, 
                                                         center=False, 
                                                         scale=False)
        
        if initializer is None:
            self.initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, initializer):
            self.initializer = getattr(tf.keras.initializers, initializer)
        else:
            raise ValueError(f"Could not find initializer <{initializer}> in tf.keras.initializers...")
        
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        
        self.L = L
        if self.L:
            self.sparse_L = L
        else:
            self.sparse_L = get_sparse_L(nside=self.nside, indices=self.indices, n_neighbors = self.n_neighbors)
        
        self.kwargs = kwargs
    
    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        :return: the kernel variable to train
        """

        # get the input shape
        Fin = int(input_shape[-1])

        self.kernel = self.add_weight("kernel", 
                                      shape=[Fin, self.depth_multiplier, (self.K-self.K0)],
                                      trainable=True,
                                      initializer=self.initializer, 
                                      regularizer=self.kernel_regularizer,
                                      **self.kwargs)

        if self.use_bias:
            self.bias = self.add_weight("bias", 
                                        shape=[1, 1, dFout],
                                        trainable=True)

        # we cast the sparse L to the current backend type
        if tf.keras.backend.floatx() == 'float16':
            self.sparse_L = tf.cast(self.sparse_L, tf.float16)
        if tf.keras.backend.floatx() == 'float32':
            self.sparse_L = tf.cast(self.sparse_L, tf.float32)
        if tf.keras.backend.floatx() == 'float64':
            self.sparse_L = tf.cast(self.sparse_L, tf.float64)

    def call(self, input_tensor, training=True, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param training: wheter we are training or not
        :param kwargs: further keyword arguments
        :return: the output of the layer
        """

        # shapes, this fun is necessary since sparse_matmul_dense in TF only supports
        # the multiplication of 2d matrices, therefore one has to do some weird reshaping
        # this is not strictly necessary but leads to a huge performance gain...
        # See: https://arxiv.org/pdf/1903.11409.pdf
        N, M, Fin = input_tensor.get_shape()
        M, Fin = int(M), int(Fin)


        # Transform to Legendre basis, x0 -> P_0 -> P_0(L) = I
        x0 = tf.transpose(input_tensor, perm=[1, 0, 2])  # M x N x Fin
        x0 = tf.reshape(x0, [M, -1])  # M x N*Fin

        # list for stacking
        stack = [x0]  #P_0(L) = 1
        if self.K > 1:
            x1 = tf.sparse.sparse_dense_matmul(self.sparse_L, x0) #P_1(L) = L
            stack.append(x1)
        for k in range(2, self.K): #Chebyshev polynomial recursion rule
            x2 = 2 * tf.sparse.sparse_dense_matmul(self.sparse_L, x1) - x0  # M x Fin*N
            stack.append(x2)
            x0, x1 = x1, x2
        
        #depthwise step
        x = tf.stack(stack, axis=0)[self.K0:] #(K-K0) x M x N*Fin
        x = tf.reshape(x, [(self.K-self.K0), M, -1, Fin])  # (K-K0) x M x N x Fin
        x = tf.transpose(x, perm=[3, 0, 2, 1]) # Fin x (K-K0) x N x M
        x = tf.reshape(x, [Fin, self.K-self.K0, -1]) # Fin x (K-K0) x N*M
        #depthwise kernel shape is Fin x depth_multiplier x (K-K0)
        #there are depth_multiplier many kernels of size K-K0 for each Fin
        x = tf.matmul(self.kernel, x)  # Fin x depth_multiplier x N*M  this uses tf.matmul batch multiplication rules
        x = tf.transpose(x, [2,0,1]) # N*M x Fin x depth_multiplier
        x = tf.reshape(x, [-1, M, Fin*self.depth_multiplier])

        if self.use_bias:
            x = tf.add(x, self.bias)

        if self.activation is not None:
            x = self.activation(x)
        
        if self.use_bn:
            x = self.bn(x, training=training)

        return x

    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  #"n_neighbors":self.n_neighbors,
                  "K":self.K,
                  "K0":self.K0,
                  "depth_multiplier":self.dFout,
                  "initializer":self.initializer,
                  "activation":self.activation,
                  "use_bias":self.use_bias,
                  "use_bn":self.use_bn,
                  "kernel_regularizer":self.kernel_regularizer,
                  "poly_type":"legendre",
                  #"L":self.L,
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))
    
    
class HealpySeparableLegendreConv(Layer):
    """
    A graph convolutional layer using the Legendre approximation
    """

    def __init__(self, nside, indices, n_neighbors, K, Fout, K0=0, depth_multiplier=1, 
                 pointwise_initializer='he_normal', depthwise_initializer='he_normal', activation=None, 
                 use_bias=False, use_bn=False, pwise_kernel_regularizer=None, 
                 dwise_kernel_regularizer=None, L=None, **kwargs):
        """
        Initializes the graph convolutional layer, assuming the input has dimension (B, M, F)
        :param nside: nside of the input map
        :param indices: indices of the input map
        :param n_neighbors: n_neighbors of the graph laplacian
        :param K: Order of the polynomial to use
        :param Fout: Number of features of the output
        :param K0: if specified, will reduce the kernel size to K-K0 and will only consider the last K-K0 
                   legendre polynomials. K0 < K. defaults to 0
        :param depth_multiplier: Number of depthwise features (channels) of the output, defaults to 1
                      given an input tensor of shape (batch, num_pixels, channels), the output
                      will be of shape (batch, num_pixels, dFout*channels) before the pointwie convolution
        :param pointwise_initializer: initializer to use for weight initialisation of the pointwise kernel
        :param depthwise_initializer: initializer to use for weight initialisation of the depthwise kernel
        :param activation: the activation function to use after the layer, defaults to linear
        :param use_bias: Use learnable bias weights
        :param use_bn: Apply batch norm before adding the bias
        :param pwise_kernel_regularizer: kernel regularizer for the pointwise convolution
        :param dwise_kernel_regularizer: kernel regularizer for the depthwise convolution
        :param L: Optional. graph Laplacian L of shape indices*incides. if included it will be used directly instead
                  of calculating it from nside and indices. either L or (nside, indices, n_neighbors) need to be
                  given.
        :param kwargs: additional keyword arguments passed on to add_weight
        """

        # This is necessary for every Layer
        super(HealpySeparableLegendreConv, self).__init__(**kwargs)

        # save necessary parameters
        self.nside = nside
        self.indices = indices
        self.n_neighbors = n_neighbors
        self.K = K
        self.K0 = K0
        self.Fout = Fout
        self.depth_multiplier = depth_multiplier
        self.pointwise_initializer = pointwise_initializer
        self.depthwise_initializer = depthwise_initializer
        self.use_bias = use_bias
        self.use_bn = use_bn
        self.pwise_kernel_regularizer = pwise_kernel_regularizer
        self.dwise_kernel_regularizer = dwise_kernel_regularizer
        if self.use_bn:
            self.bn = tf.keras.layers.BatchNormalization(axis=-1, 
                                                         momentum=0.9, 
                                                         epsilon=1e-5, 
                                                         center=False, 
                                                         scale=False)
        if pointwise_initializer is None:
            self.pointwise_initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, pointwise_initializer):
            self.initializer = getattr(tf.keras.initializers, pointwise_initializer)
        else:
            raise ValueError(f"Could not find initializer <{pointwise_initializer}> in tf.keras.initializers...")
        
        if depthwise_initializer is None:
            self.depthwise_initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, depthwise_initializer):
            self.initializer = getattr(tf.keras.initializers, depthwise_initializer)
        else:
            raise ValueError(f"Could not find initializer <{depthwise_initializer}> in tf.keras.initializers...")
                
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
            
        self.L = L
        if self.L:
            self.sparse_L = L #needs to be a tf.sparsetensor, there's a function defined for this purpose
        else:
            self.sparse_L = get_sparse_L(nside=self.nside, indices=self.indices, n_neighbors = self.n_neighbors)
        
        self.kwargs = kwargs
        
    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        :return: the kernel variable to train
        """

        # get the input shape
        Fin = int(input_shape[-1])

        # get Fout if necessary
        if self.Fout is None:
            Fout = self.depth_multiplier*Fin
        else:
            Fout = self.Fout
        #dFout = self.depth_multiplier

        self.pkernel = self.add_weight("pointwise_kernel", 
                                       shape=[Fout, Fin*self.depth_multiplier],
                                       trainable=True,
                                       initializer=self.pointwise_initializer, 
                                       regularizer=self.pwise_kernel_regularizer,
                                       **self.kwargs)

        self.dkernel = self.add_weight("depthwise_kernel", 
                                       shape=[Fin, self.depth_multiplier, self.K - self.K0],
                                       trainable=True,
                                       initializer=self.depthwise_initializer, 
                                       regularizer=self.dwise_kernel_regularizer,
                                       **self.kwargs)             

        if self.use_bias:
            self.bias = self.add_weight("bias", 
                                        shape=[1, 1, Fout],
                                        trainable=True)

        # we cast the sparse L to the current backend type
        if tf.keras.backend.floatx() == 'float16':
            self.sparse_L = tf.cast(self.sparse_L, tf.float16)
        if tf.keras.backend.floatx() == 'float32':
            self.sparse_L = tf.cast(self.sparse_L, tf.float32)
        if tf.keras.backend.floatx() == 'float64':
            self.sparse_L = tf.cast(self.sparse_L, tf.float64)

    def call(self, input_tensor, training=True, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param training: wheter we are training or not
        :param kwargs: further keyword arguments
        :return: the output of the layer
        """

        # get Fout if necessary
        if self.Fout is None:
            Fout = self.depth_multiplier*Fin
        else:
            Fout = self.Fout
        N, M, Fin = input_tensor.get_shape()
        M, Fin = int(M), int(Fin)

        x0 = tf.transpose(input_tensor, perm=[1, 0, 2])  # M x N x Fin
        x0 = tf.reshape(x0, [M, -1])  # M x N*Fin

        # list for stacking
        stack = [x0]  #P_0(L) = 1
        if self.K > 1:
            x1 = tf.sparse.sparse_dense_matmul(self.sparse_L, x0) #P_1(L) = L
            stack.append(x1)
        for k in range(2, self.K): #legendre polynomial recursion rule P_k = (2*k-1)/k * x * P_{k-1} - (k-1)/k * P_{k-2}
            x2 = ((2*k-1)/k) * tf.sparse.sparse_dense_matmul(self.sparse_L, x1) - ((k-1)/k)*x0  # M x Fin*N
            stack.append(x2)
            x0, x1 = x1, x2
        
        #depthwise step
        x = tf.stack(stack, axis=0)[self.K0:] #(K-K0) x M x N*Fin
        x = tf.reshape(x, [(self.K-self.K0), M, -1, Fin])  # (K-K0) x M x N x Fin
        x = tf.transpose(x, perm=[3, 0, 2, 1]) # Fin x (K-K0) x N x M
        x = tf.reshape(x, [Fin, self.K-self.K0, -1]) # Fin x (K-K0) x N*M
        #depthwise kernel shape is Fin x depth_multiplier x (K-K0)
        #there are dFout many kernels of size K-K0 for each Fin
        x = tf.matmul(self.dkernel, x)  # Fin x depth_multiplier x N*M  this uses tf.matmul batch multiplication rules

        #pointwise conv step
        #since this is a conv with an order 0 polynomial, we don't need to go thru the polynomials again and just set
        #x = x0 and repeat the usual conv but just on x0 = input map
        
        x = tf.reshape(x, [Fin*self.depth_multiplier, -1]) # Fin*dFout x N*M
        x = tf.matmul(self.pkernel, x) # Fout x N*M
        x = tf.transpose(x) #N*M x Fout
        x = tf.reshape(x, [-1,M,Fout]) # N x M x Fout

        if self.use_bias:
            x = tf.add(x, self.bias)

        if self.activation is not None:
            x = self.activation(x)
            
        if self.use_bn:
            x = self.bn(x, training=training)

        return x    

    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  #"n_neighbors":self.n_neighbors,
                  "K":self.K,
                  "K0":self.K0,
                  "Fout":self.Fout,
                  "depth_multiplier":self.depth_multiplier,
                  "pointwise_initializer":self.pointwise_initializer,
                  "depthwise_initializer":self.depthwise_initializer,
                  "activation":self.activation,
                  "use_bias":self.use_bias,
                  "use_bn":self.use_bn,
                  "pwise_kernel_regularizer":self.pwise_kernel_regularizer,
                  "dwise_kernel_regularizer":self.dwise_kernel_regularizer,
                  "poly_type":"legendre",
                  #"L":self.L,
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

    
class HealpySeparableChebyshevConv(Layer):
    """
    A graph convolutional layer using the Legendre approximation
    """

    def __init__(self, nside, indices, n_neighbors, K, Fout, K0=0, depth_multiplier=1, 
                 pointwise_initializer='he_normal', depthwise_initializer='he_normal', activation=None, 
                 use_bias=False, use_bn=False, pwise_kernel_regularizer=None, 
                 dwise_kernel_regularizer=None, L=None, **kwargs):
        """
        Initializes the graph convolutional layer, assuming the input has dimension (B, M, F)
        :param nside: nside of the input map
        :param indices: indices of the input map
        :param n_neighbors: n_neighbors of the graph laplacian
        :param K: Order of the polynomial to use
        :param Fout: Number of features of the output
        :param K0: if specified, will reduce the kernel size to K-K0 and will only consider the last K-K0 
                   legendre polynomials. K0 < K. defaults to 0
        :param depth_multiplier: Number of depthwise features (channels) of the output, defaults to 1
                      given an input tensor of shape (batch, num_pixels, channels), the output
                      will be of shape (batch, num_pixels, dFout*channels) before the pointwie convolution
        :param pointwise_initializer: initializer to use for weight initialisation of the pointwise kernel
        :param depthwise_initializer: initializer to use for weight initialisation of the depthwise kernel
        :param activation: the activation function to use after the layer, defaults to linear
        :param use_bias: Use learnable bias weights
        :param use_bn: Apply batch norm before adding the bias
        :param pwise_kernel_regularizer: kernel regularizer for the pointwise convolution
        :param dwise_kernel_regularizer: kernel regularizer for the depthwise convolution
        :param L: Optional. graph Laplacian L of shape indices*incides. if included it will be used directly instead
                  of calculating it from nside and indices. either L or (nside, indices, n_neighbors) need to be
                  given.
        :param kwargs: additional keyword arguments passed on to add_weight
        """

        # This is necessary for every Layer
        super(HealpySeparableChebyshevConv, self).__init__(**kwargs)

        # save necessary parameters
        self.nside = nside
        self.indices = indices
        self.n_neighbors = n_neighbors
        self.K = K
        self.K0 = K0
        self.Fout = Fout
        self.depth_multiplier = depth_multiplier
        self.pointwise_initializer = pointwise_initializer
        self.depthwise_initializer = depthwise_initializer
        self.use_bias = use_bias
        self.use_bn = use_bn
        self.pwise_kernel_regularizer = pwise_kernel_regularizer
        self.dwise_kernel_regularizer = dwise_kernel_regularizer
        if self.use_bn:
            self.bn = tf.keras.layers.BatchNormalization(axis=-1, 
                                                         momentum=0.9, 
                                                         epsilon=1e-5, 
                                                         center=False, 
                                                         scale=False)
        if pointwise_initializer is None:
            self.pointwise_initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, pointwise_initializer):
            self.initializer = getattr(tf.keras.initializers, pointwise_initializer)
        else:
            raise ValueError(f"Could not find initializer <{pointwise_initializer}> in tf.keras.initializers...")
        
        if depthwise_initializer is None:
            self.depthwise_initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, depthwise_initializer):
            self.initializer = getattr(tf.keras.initializers, depthwise_initializer)
        else:
            raise ValueError(f"Could not find initializer <{depthwise_initializer}> in tf.keras.initializers...")
                
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
            
        self.L = L
        if self.L:
            self.sparse_L = L #needs to be a tf.sparsetensor, there's a function defined for this purpose
        else:
            self.sparse_L = get_sparse_L(nside=self.nside, indices=self.indices, n_neighbors = self.n_neighbors)
        
        self.kwargs = kwargs
        
    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        :return: the kernel variable to train
        """

        # get the input shape
        Fin = int(input_shape[-1])

        # get Fout if necessary
        if self.Fout is None:
            Fout = self.depth_multiplier*Fin
        else:
            Fout = self.Fout
        #dFout = self.depth_multiplier

        self.pkernel = self.add_weight("pointwise_kernel", 
                                       shape=[Fout, Fin*self.depth_multiplier],
                                       trainable=True,
                                       initializer=self.pointwise_initializer, 
                                       regularizer=self.pwise_kernel_regularizer,
                                       **self.kwargs)

        self.dkernel = self.add_weight("depthwise_kernel", 
                                       shape=[Fin, self.depth_multiplier, self.K - self.K0],
                                       trainable=True,
                                       initializer=self.depthwise_initializer, 
                                       regularizer=self.dwise_kernel_regularizer,
                                       **self.kwargs)             

        if self.use_bias:
            self.bias = self.add_weight("bias", 
                                        shape=[1, 1, Fout],
                                        trainable=True)

        # we cast the sparse L to the current backend type
        if tf.keras.backend.floatx() == 'float16':
            self.sparse_L = tf.cast(self.sparse_L, tf.float16)
        if tf.keras.backend.floatx() == 'float32':
            self.sparse_L = tf.cast(self.sparse_L, tf.float32)
        if tf.keras.backend.floatx() == 'float64':
            self.sparse_L = tf.cast(self.sparse_L, tf.float64)

    def call(self, input_tensor, training=True, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param training: wheter we are training or not
        :param kwargs: further keyword arguments
        :return: the output of the layer
        """

        # get Fout if necessary
        if self.Fout is None:
            Fout = self.depth_multiplier*Fin
        else:
            Fout = self.Fout
        N, M, Fin = input_tensor.get_shape()
        M, Fin = int(M), int(Fin)

        x0 = tf.transpose(input_tensor, perm=[1, 0, 2])  # M x N x Fin
        x0 = tf.reshape(x0, [M, -1])  # M x N*Fin

        # list for stacking
        stack = [x0]  #P_0(L) = 1
        if self.K > 1:
            x1 = tf.sparse.sparse_dense_matmul(self.sparse_L, x0) #P_1(L) = L
            stack.append(x1)
        for k in range(2, self.K): #Chebyshev polynomials recursion rule P_k = 2 * x * P_{k-1} -  P_{k-2}
            x2 = 2 * tf.sparse.sparse_dense_matmul(self.sparse_L, x1) - x0  # M x Fin*N
            stack.append(x2)
            x0, x1 = x1, x2
        
        #depthwise step
        x = tf.stack(stack, axis=0)[self.K0:] #(K-K0) x M x N*Fin
        x = tf.reshape(x, [(self.K-self.K0), M, -1, Fin])  # (K-K0) x M x N x Fin
        x = tf.transpose(x, perm=[3, 0, 2, 1]) # Fin x (K-K0) x N x M
        x = tf.reshape(x, [Fin, self.K-self.K0, -1]) # Fin x (K-K0) x N*M
        #depthwise kernel shape is Fin x depth_multiplier x (K-K0)
        #there are dFout many kernels of size K-K0 for each Fin
        x = tf.matmul(self.dkernel, x)  # Fin x depth_multiplier x N*M  this uses tf.matmul batch multiplication rules

        #pointwise conv step
        #since this is a conv with an order 0 polynomial, we don't need to go thru the polynomials again and just set
        #x = x0 and repeat the usual conv but just on x0 = input map
        
        x = tf.reshape(x, [Fin*self.depth_multiplier, -1]) # Fin*dFout x N*M
        x = tf.matmul(self.pkernel, x) # Fout x N*M
        x = tf.transpose(x) #N*M x Fout
        x = tf.reshape(x, [-1,M,Fout]) # N x M x Fout

        if self.use_bias:
            x = tf.add(x, self.bias)

        if self.activation is not None:
            x = self.activation(x)

        if self.use_bn:
            x = self.bn(x, training=training)
        
        return x    

    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  #"n_neighbors":self.n_neighbors,
                  "K":self.K,
                  "K0":self.K0,
                  "Fout":self.Fout,
                  "depth_multiplier":self.depth_multiplier,
                  "pointwise_initializer":self.pointwise_initializer,
                  "depthwise_initializer":self.depthwise_initializer,
                  "activation":self.activation,
                  "use_bias":self.use_bias,
                  "use_bn":self.use_bn,
                  "pwise_kernel_regularizer":self.pwise_kernel_regularizer,
                  "dwise_kernel_regularizer":self.dwise_kernel_regularizer,
                  "poly_type":"legendre",
                  #"L":self.L,
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))
    
class HealpyMonomialConv(Layer):
    """
    A graph convolutional layer using Monomials
    """
    def __init__(self, nside, indices, n_neighbors, K, Fout=None, initializer=None, activation=None, use_bias=False,
                 use_bn=False, kernel_regularizer=None, **kwargs):
        """
        Initializes the graph convolutional layer, assuming the input has dimension (B, M, F)
        :param nside: nside of the input map
        :param indices: indices of the input map
        :param n_neighbors: n_neighbors of the graph laplacian
        :param K: Order of the polynomial to use
        :param Fout: Number of features (channels) of the output, default to number of input channels
        :param initializer: initializer to use for weight initialisation
        :param activation: the activation function to use after the layer, defaults to linear
        :param use_bias: Use learnable bias weights
        :param use_bn: Apply batch norm before adding the bias
        :param kernel_regularizer: kernel regularizer
        :param kwargs: additional keyword arguments passed on to add_weight
        """

        # This is necessary for every Layer
        super(HealpyMonomialConv, self).__init__()

        # save necessary params
        self.nside = nside
        self.indices = indices
        self.n_neighbors = n_neighbors
        self.K = K
        self.Fout = Fout
        self.use_bias = use_bias
        self.use_bn = use_bn
        self.kernel_regularizer = kernel_regularizer
        if self.use_bn:
            self.bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5, center=False, scale=False)
        
        if initializer is None:
            self.initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, initializer):
            self.initializer = getattr(tf.keras.initializers, initializer)
        else:
            raise ValueError(f"Could not find initializer <{initializer}> in tf.keras.initializers...")
        
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        
        self.L = L
        if self.L:
            self.sparse_L = L
        else:
            self.sparse_L = get_sparse_L(nside=self.nside, indices=self.indices, n_neighbors = self.n_neighbors)
        
        self.kwargs = kwargs

    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        """

        # get the input shape
        Fin = int(input_shape[-1])

        # get Fout if necessary
        if self.Fout is None:
            Fout = Fin
        else:
            Fout = self.Fout

        self.kernel = self.add_weight("kernel", shape=[self.K * Fin, Fout],
                                      initializer=self.initializer, 
                                      regularizer=self.kernel_regularizer,
                                      trainable=True,
                                      **self.kwargs)

        if self.use_bias:
            self.bias = self.add_weight("bias",
                                        shape=[1, 1, Fout],
                                        trainable=True)

        # we cast the sparse L to the current backend type
        if tf.keras.backend.floatx() == 'float16':
            self.sparse_L = tf.cast(self.sparse_L, tf.float16)
        if tf.keras.backend.floatx() == 'float32':
            self.sparse_L = tf.cast(self.sparse_L, tf.float32)
        if tf.keras.backend.floatx() == 'float64':
            self.sparse_L = tf.cast(self.sparse_L, tf.float64)

    def call(self, input_tensor, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param training: wheter we are training or not
        :param args: further arguments
        :param kwargs: further keyword arguments
        :return: the output of the layer
        """

        # shapes, this fun is necessary since sparse_matmul_dense in TF only supports
        # the multiplication of 2d matrices, therefore one has to do some weird reshaping
        # this is not strictly necessary but leads to a huge performance gain...
        # See: https://arxiv.org/pdf/1903.11409.pdf
        N, M, Fin = input_tensor.get_shape()
        M, Fin = int(M), int(Fin)

        # get Fout if necessary
        if self.Fout is None:
            Fout = Fin
        else:
            Fout = self.Fout

        # Transform to monomial basis.
        x0 = tf.transpose(input_tensor, perm=[1, 2, 0])  # M x Fin x N
        x0 = tf.reshape(x0, [M, -1])  # M x Fin*N

        # list for stacking
        stack = [x0]

        for k in range(1, self.K):
            x1 = tf.sparse.sparse_dense_matmul(self.sparse_L, x0)  # M x Fin*N
            stack.append(x1)
            x0 = x1

        x = tf.stack(stack, axis=0)
        x = tf.reshape(x, [self.K, M, Fin, -1])  # K x M x Fin x N
        x = tf.transpose(x, perm=[3, 1, 2, 0])  # N x M x Fin x K
        x = tf.reshape(x, [-1, Fin * self.K])  # N*M x Fin*K
        # Filter: Fin*Fout filters of order K, i.e. one filterbank per output feature.
        x = tf.matmul(x, self.kernel)  # N*M x Fout
        x = tf.reshape(x, [-1, M, Fout])  # N x M x Fout

        if self.use_bias:
            x = tf.add(x, self.bias)

        if self.activation is not None:
            x = self.activation(x)
        
        if self.use_bn:
            x = self.bn(x, training=training)

        return x

    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  #"n_neighbors":self.n_neighbors,
                  "K":self.K,
                  "Fout":self.Fout,
                  "initializer":self.initializer,
                  "activation":self.activation,
                  "use_bias":self.use_bias,
                  "use_bn":self.use_bn,
                  "kernel_regularizer":self.kernel_regularizer,
                  "poly_type":"mono",
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

class HealpyPool(Layer):
    """
    A pooling layer for healpy maps, makes use of the fact that a pixels is always divided into 4 subpixels when
    increasing the nside of a HealPix map
    
    outputs: the output tensor, nside of the output map, indices of the output map
    """

    def __init__(self, nside, indices, p, pool_type="MAX", **kwargs):
        """
        initializes the layer
        :param nside: nside of the input map
        :param indices: np.array of indices of the map
        :param p: reduction factor >=1 of the nside -> number of nodes reduces by 4^p, note that the layer only checks
                  if the dimensionality of the input is evenly divisible by 4^p and not if the ordering is correct
                  (should be nested ordering)
        :param pool_type: type of pooling, can be "MAX" or  "AVG"
        :param kwargs: additional kwargs passed to the keras pooling layer
        """
        
        
        # This is necessary for every Layer
        super(HealpyPool, self).__init__()

        # check p
        if not p >= 1:
            raise IOError("The reduction factors has to be at least 2!")

        # save variables
        self.nside = nside
        self.indices = indices
        self.p = p
        self.filter_size = int(4**p)
        self.pool_type = pool_type
        self.kwargs = kwargs
        self.nside_out = self.nside//(2**self.p)
        #self.masked_indices = reduce_indices(nside_in=self.nside,     #old one relying on healpy
        #                                     nside_out=self.nside_out, 
        #                                     indices_in=self.indices)
        self.masked_indices = reduce_indices_v2(indices=self.indices,  #the new index function
                                                p=self.p)
        
        
        #self.indices_out = transformed_indices(self.nside,            #old index transformation func, relying on healpy
        #                                       self.nside_out, 
        #                                       self.masked_indices)
        
        self.indices_out = transformed_indices_v2(indices=self.masked_indices,  #new index function
                                                  p=self.p,
                                                  reduce=True)
        
        #self.indices_out = transformed_indices(self.nside, self.nside_out, self.indices)

        if pool_type == "MAX":
            self.filter = tf.keras.layers.MaxPool1D(pool_size=self.filter_size, strides=self.filter_size,
                                                    padding='valid', data_format='channels_last', **kwargs)
        elif pool_type == "AVG":
            self.filter = tf.keras.layers.AveragePooling1D(pool_size=self.filter_size, strides=self.filter_size,
                                                           padding='valid', data_format='channels_last', **kwargs)
        else:
            raise IOError(f"Pooling type not understood: {self.pool_type}")

    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        """

        #n_nodes = int(input_shape[1])
        #if n_nodes % self.filter_size != 0:
        #    raise IOError("Input shape {input_shape} not compatible with the filter size {self.filter_size}")

    def call(self, input_tensor, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param kwargs: further keyword arguments
        :return: the output of the layer, nside of the output, indices of the output
        """
        x = tf.gather(input_tensor, indices=self.masked_indices, axis=1) #masking
        return self.filter(x)
    
    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  "p":self.p,
                  "filter_size":self.filter_size,
                  "pool_type":self.pool_type,
                  #"nside_out": self.nside_out,
                  #"indices_out":self.indices_out
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

class HealpyPseudoConv(Layer):
    """
    A pseudo convolutional layer on Healpy maps. It makes use of the Healpy pixel scheme and reduces the nside by
    averaging the pixels into bigger pixels using learnable weights
    """

    def __init__(self, p, Fout, activation=None, initializer=None, kernel_regularizer=None, 
                 nside=None, indices=None, **kwargs):
        """
        initializes the layer

        :param p: reduction factor >=1 of the nside -> number of nodes reduces by 4^p, note that the layer only checks
                  if the dimensionality of the input is evenly divisible by 4^p and not if the ordering is correct
                  (should be nested ordering)
        :param Fout: number of output channels
        :param activation:
        :param kernel_initializer: initializer for kernel init
        :param kernel_regularizer: a kernel regularizer from tf.keras.regularizers
        :param nside: nside of the input maps
        :param indices: indices of the input maps
        :param kwargs: additional keyword arguments passed to the keras 1D conv layer
        """
        # This is necessary for every Layer
        super(HealpyPseudoConv, self).__init__()

        # check p
        if not p >= 1:
            raise IOError("The reduction factors has to be at least 1!")

        # save variables
        self.p = p
        self.Fout = Fout
        self.filter_size = int(4 ** p)
        self.kernel_regularizer = kernel_regularizer
        
        if initializer is None:
            self.initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, initializer):
            self.initializer = getattr(tf.keras.initializers, initializer)
        else:
            raise ValueError(f"Could not find initializer <{initializer}> in tf.keras.initializers...")        
        
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        
        self.nside = nside
        self.indices = indices
        self.kwargs = kwargs
        self.nside_out = self.nside//(2**self.p)
        
        #self.masked_indices = reduce_indices(nside_in=self.nside,       #old index func, healpy backend.
        #                                     nside_out=self.nside_out, 
        #                                     indices_in=self.indices)
        
        self.masked_indices = reduce_indices_v2(indices=self.indices,    #new func
                                                p=self.p)
        
        #self.indices_out = transformed_indices(self.nside,              #old index func, healpy backend.
        #                                       self.nside_out, 
        #                                       self.masked_indices)
        
        self.indices_out = transformed_indices_v2(indices=self.masked_indices, #new func
                                                  p=self.p,
                                                  reduce=True)
        
        ######self.indices_out = transformed_indices(self.nside, self.nside_out, self.indices)  #oldold func, should remove

        # create the filters
        self.filter = tf.keras.layers.Conv1D(self.Fout, 
                                             self.filter_size, 
                                             strides=self.filter_size,
                                             padding='valid', 
                                             data_format='channels_last',
                                             kernel_initializer = self.initializer,
                                             kernel_regularizer = self.kernel_regularizer,
                                             **self.kwargs)

    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        """

        #n_nodes = int(input_shape[1])
        #if n_nodes % self.filter_size != 0:
        #    raise IOError(f"Input shape {input_shape} not compatible with the filter size {self.filter_size}")
        self.filter.build(input_shape)

    def call(self, input_tensor, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param kwargs: further keyword arguments
        :return: the output of the layer, nside of the output, indices of the output
        """
        x = tf.gather(input_tensor, indices=self.masked_indices, axis=1)
        x = self.filter(x)
        
        if self.activation is not None:
            x = self.activation(x)
        
        return x
    
    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  "p":self.p,
                  "filter_size":self.filter_size,
                  "Fout":self.Fout,
                  "initializer":self.initializer,
                  "kernel_regularizer":self.kernel_regularizer,
                  #"nside_out": self.nside_out,
                  #"indices_out":self.indices_out
                  }
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))
    

    
class HealpyPseudoConv_Transpose(Layer):
    """
    A pseudo transpose convolutional layer on Healpy maps. It makes use of the Healpy pixel scheme and increases
    the nside by applying a transpose convolution to the pixels into bigger pixels using learnable weights
    """

    def __init__(self, p, Fout, activation=None, kernel_initializer='he_normal', kernel_regularizer=None, 
                 nside=None, indices=None, **kwargs):
        """
        initializes the layer
        :param nside: optional. nside of the input maps. if given, used to calcualate the nside of the output maps
                      nside_out can be accessed by HealpyPseudoConv_Transpose.nside_out
        :param indices: optional. indices of the input maps if given, used to calcualate the indices of the output maps
                        nside_out can be accessed by HealpyPseudoConv_Transpose.indices_out
        :param p: Boost factor >=1 of the nside -> number of nodes increases by 4^p, note that the layer only checks
                  if the dimensionality of the input is evenly divisible by 4^p and not if the ordering is correct
                  (should be nested ordering)
        :param Fout: number of output channels
        :param kernel_initializer: initializer for kernel init
        :param kernel_regularizer: kernel regularizer
        :param kwargs: additional keyword arguments passed to the keras transpose conv layer
        """
        # This is necessary for every Layer
        super(HealpyPseudoConv_Transpose, self).__init__()

        # check p
        if not p >= 1:
            raise IOError("The boost factors has to be at least 1!")

        # save variables
        self.nside = nside
        self.indices = indices
        self.p = p
        self.filter_size = int(4 ** p)
        self.Fout = Fout
        self.kernel_regularizer = kernel_regularizer
        
        
        if kernel_initializer is None:
            self.kernel_initializer = tf.keras.initializers.HeNormal()
        elif hasattr(tf.keras.initializers, kernel_initializer):
            self.kernel_initializer = getattr(tf.keras.initializers, kernel_initializer)
        else:
            raise ValueError(f"Could not find initializer <{kernel_initializer}> in tf.keras.initializers...")        
                
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        self.kwargs = kwargs
        if self.nside and self.indices is not None:
            self.nside_out = self.nside*(2**self.p)
            self.indices_out = transformed_indices(self.nside, self.nside_out, self.indices) 

        # create the filters
        #self.filter = tf.keras.layers.Conv2DTranspose(self.Fout, (1, self.filter_size), strides=(1, self.filter_size),
        #                                              padding='valid', data_format='channels_last',
        #                                              kernel_initializer=self.initializer, 
        #                                              kernel_regularizer = self.kernel_regularizer,
        #                                              **self.kwargs)
        self.filter = tf.keras.layers.Conv1DTranspose(self.Fout, self.filter_size, strides=self.filter_size,
                                                      padding='valid', data_format='channels_last',
                                                      kernel_initializer = self.kernel_initializer, 
                                                      kernel_regularizer = self.kernel_regularizer,
                                                      **self.kwargs)
    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        """

        input_shape = list(input_shape)
        n_nodes = input_shape[1]
        #if n_nodes % self.filter_size != 0:
        #    raise IOError(f"Input shape {input_shape} not compatible with the filter size {self.filter_size}")

        # add the additional dim
        #input_shape.insert(1, 1)

        self.filter.build(input_shape)

    def call(self, input_tensor, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param kwargs: further keyword arguments
        :return: the output of the layer, nside of the output, indices of the output
        """       
        #input_tensor = tf.expand_dims(input_tensor, axis=1)
        #x = tf.squeeze(self.filter(input_tensor), axis=1)
        x = self.filter(input_tensor)
        
        if self.activation is not None:
            x = self.activation(x)

        return x

    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  "p":self.p,
                  "filter_size":self.filter_size,
                  "Fout":self.Fout,
                  "kernel_initializer":self.initializer,
                  "kernel_regularizer":self.kernel_regularizer,
                  #"nside_out":self.nside_out,
                  #"indices_out":self.indices_out,
                  "kwargs":self.kwargs}
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

class HealpyPseudoSeparableConv(Layer):
    """
    A pseudo separable convolutional layer on Healpy maps. It makes use of the Healpy pixel scheme and reduces the nside by
    averaging the pixels into bigger pixels using learnable weights
    """

    def __init__(self, p, Fout, depthwise_Fout, activation=None,
                 kernel_initializer='he_normal', kernel_regularizer=None, nside=None, indices=None, **kwargs):
        """
        initializes the layer
        :param nside: optional. nside of the input maps. if given, used to calculate nside_out
                      accessible by HealpyPseudoSeparableConv(..).nside_out
        :param indices: optional. indices of the input maps. if given, used to calculate indices_out
                        accessible by HealpyPseudoSeparableConv(..).indices_out
        :param p: reduction factor >=1 of the nside -> number of nodes reduces by 4^p, note that the layer only checks
                  if the dimensionality of the input is evenly divisible by 4^p and not if the ordering is correct
                  (should be nested ordering)
        :param Fout: number of output channels
        :param depthwise_Fout: number of depthwise filters, passed to the depthwise multiplier arg  
        :param activation: activation to be applied to the output of the filter
        :param kernel_initializer: initializer for kernel init
        :param kernel_regularizer: a kernel regularizer from tf.keras.regularizers
        :param kwargs: additional keyword arguments passed to the keras 1D separable conv layer
        """
        # This is necessary for every Layer
        super(HealpyPseudoSeparableConv, self).__init__()

        # check p
        if not p >= 1:
            raise IOError("The reduction factors has to be at least 1!")

        # save variables
        self.nside = nside
        self.indices = indices
        self.p = p
        self.filter_size = int(4 ** p)
        self.Fout = Fout
        self.depthwise_Fout = depthwise_Fout
        self.kernel_initializer = kernel_initializer
        self.kernel_regularizer = kernel_regularizer
        self.kwargs = kwargs
        if self.nside and self.indices is not None:
            self.nside_out = self.nside//(2**self.p)
            self.masked_indices = reduce_indices(nside_in=self.nside, 
                                             nside_out=self.nside_out, 
                                             indices_in=self.indices)
            self.indices_out = transformed_indices(self.nside, 
                                               self.nside_out, 
                                               self.masked_indices)
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        self.kwargs = kwargs
        
        

        # create the filters
        self.filter = tf.keras.layers.SeparableConv1D(self.Fout, self.filter_size, strides=self.filter_size,
                                                      padding='valid', data_format='channels_last',
                                                      depth_multiplier = self.depthwise_Fout,
                                                      activation = self.activation,
                                                      kernel_initializer=self.kernel_initializer,
                                                      kernel_regularizer = self.kernel_regularizer,
                                                      **self.kwargs)

    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        """

        n_nodes = int(input_shape[1])
        if n_nodes % self.filter_size != 0:
            raise IOError(f"Input shape {input_shape} not compatible with the filter size {self.filter_size}")
        self.filter.build(input_shape)

    def call(self, input_tensor, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param kwargs: further keyword arguments
        :return: the output of the layer 
        """
        return self.filter(input_tensor)
    
    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  "p":self.p,
                  "filter_size":self.filter_size,
                  "Fout":self.Fout,
                  "depthwise_Fout":self.depthwise_Fout,
                  "kernel_initializer":self.initializer,
                  "kernel_regularizer":self.kernel_regularizer,
                  "activation":self.activation,
                  #"nside_out": self.nside_out,
                  #"indices_out":self.indices_out,
                  "kwargs":self.kwargs 
                  }
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))
    
class HealpySeparableConv(Layer):
    """
    An implementation of depthwise separable convolutions on the Healpy sphere by repeated application
    of usampling and HealpyPseudoSeparableConv
    """
    def __init__(self, p, Fout, depthwise_Fout, activation=None,
                 kernel_initializer='he_normal', kernel_regularizer=None, **kwargs):
        """
        initializes the layer
        :param nside: nside of the input maps
        :param indices: indices of the input maps
        :param p: reduction factor >=1 of the nside -> number of nodes reduces by 4^p, note that the layer only 
                  checks if the dimensionality of the input is evenly divisible by 4^p and not if the ordering 
                  is correct (should be nested ordering)
        :param Fout: number of output channels
        :param depthwise_Fout: number of depthwise filters, passed to the depthwise multiplier arg
        :param activation: the activation function to use after the layer, defaults to linear
        :param kernel_initializer: initializer for kernel init
        :param kernel_regularizer: a kernel regularizer from tf.keras.regularizers
        :param kwargs: additional keyword arguments passed to the keras 1D separable conv layer
        """
        # This is necessary for every Layer
        super(HealpySeparableConv, self).__init__()

        # check p
        if not p >= 1:
            raise IOError("The reduction factors has to be at least 1!")

        # save variables
        self.p = p
        self.filter_size = int(4 ** p)
        self.Fout = Fout
        self.depthwise_Fout = depthwise_Fout
        self.kernel_initializer = kernel_initializer
        self.kernel_regularizer = kernel_regularizer
        if activation is None or callable(activation):
            self.activation = activation
        elif hasattr(tf.keras.activations, activation):
            self.activation = getattr(tf.keras.activations, activation)
        else:
            raise ValueError(f"Could not find activation <{activation}> in tf.keras.activations...")
        self.kwargs = kwargs

        # create the filters
        self.upsample = tf.keras.layers.UpSampling1D(size=self.filter_size)
        self.filter = tf.keras.layers.SeparableConv1D(self.Fout,
                                                      self.filter_size, 
                                                      strides=self.filter_size,
                                                      padding='valid', 
                                                      data_format='channels_last',
                                                      depth_multiplier=self.depthwise_Fout,
                                                      activation = self.activation,
                                                      kernel_initializer=self.kernel_initializer,
                                                      kernel_regularizer=self.kernel_regularizer,
                                                      **self.kwargs
                                                      )

    def build(self, input_shape):
        """
        Build the weights of the layer
        :param input_shape: shape of the input, batch dim has to be defined
        """

        #n_nodes = int(input_shape[1])
        #if n_nodes % self.filter_size != 0:
        #    raise IOError(f"Input shape {input_shape} not compatible with the filter size {self.filter_size}")
        self.upsample.build(input_shape)
        self.filter.build(input_shape)
        

    def call(self, input_tensor, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param kwargs: further keyword arguments
        :return: the output of the layer, nside of the output, indices of the output
        """
        #nside_out = nside // (2**self.p)
        #indices_out = transformed_indices(self.nside, nside_out, self.indices)
        input_tensor = self.upsample(input_tensor)
        return self.filter(input_tensor)# ,self.nside_out, self.indices_out
    
    def get_config(self):
        config = {#"nside":self.nside,
                  #"indices":self.indices,
                  "p":self.p,
                  "Fout":self.Fout,
                  "depthwise_Fout":self.depthwise_Fout,
                  "kernel_initializer":self.kernel_initializer,
                  "kernel_regularizer":self.kernel_regularizer,
                  #"nside_out": self.nside_out,
                  #"indices_out":self.indices_out,
                  "kwargs":self.kwargs
                 }
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))

class HealpyMask(Layer):
    """
    A utility layer to mask input maps.
    """
    def __init__(self, unmasked_indices, nside=None, indices=None):
        """
        :param unmasked_indices: array. indices that will be left unmasked.
        :param nside: optional, int of form 2**p, nside of the input maps
        :param indices: optional, array, indices of the input maps
        returns: a tensor of shape (N, M, F)
        """
        super(HealpyMask, self).__init__()
        self.unmasked_indices = unmasked_indices
        self.nside = nside
        self.indices = indices
        self.nside_out = nside
        self.indices_out = unmasked_indices
    
    

    def call(self, input_tensor, *args, **kwargs):
        """
        Calls the layer on a input tensor
        :param input_tensor: input of the layer shape (batch, nodes, channels)
        :param args: further arguments
        :param kwargs: further keyword arguments
        :return: the output of the layer, nside of the output, indices of the output
        """
        x = tf.gather(input_tensor, indices=self.unmasked_indices, axis=1)        
        return x














